{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Globals\n",
    "NUMBER_ITERATION = 20000\n",
    "COLLECTION_STEPS = 1\n",
    "BATCH_SIZE = 64\n",
    "EVAL_EPISODES = 10\n",
    "EVAL_INTERVAL = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported TensorFlow and a lot of modules from __TF-Agents__. \n",
    "\n",
    "One of the classes we imported is __DqnAgent__, specific agent that can perform Deep Q-Learning. This is really cool and saves us a lot of time. \n",
    "\n",
    "Also we imported __QNetwork__ class. This class is an abstraction of neural network that we use for learning. As you can see, as with transfer learning, this saves us a bunch of time. \n",
    "\n",
    "We also import __suite_gym__ and __tf_py_environment__. The first module grants us access to training environments. __Since all of these environments are implemented in Python__, we need to __wrap them up into TensorFlow__. That is what __tf_py_environment__ is used for. \n",
    "\n",
    "For _experience replay_, we use class __TFUniformReplayBuffer__ and in this buffer we store trajectories. Trajectory is a tuple that contains state of the environment in some time step, action that agent should take it in that state and state in which the environment will be after defined action is performed. \n",
    "\n",
    "# Entorno\n",
    "\n",
    "After importing all necessary modules, we need to construct the environment. In fact, we need two environments, one for training and the other one for evaluation. Here is how we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))\n",
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "train_env = suite_gym.load('CartPole-v0')\n",
    "evaluation_env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(train_env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "evaluation_env = tf_py_environment.TFPyEnvironment(evaluation_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "Now, we can _build DQN agent_. Before we proceed with that, __we need to create an instance of QNetwork class__. \n",
    "\n",
    "## QNetwork\n",
    "\n",
    "Here we have two obligatory parameters and a number of optional ones:\n",
    "\n",
    "- input_tensor_spec, which is the set of possible states of the environment \n",
    "- action_spec, which is the set of possible actions that agent can be undertake in that environment\n",
    "\n",
    "Among other parameters fc_layer_params, is of great importance to us. Using this parameters, we can define number of neurons for each hidden layer. We use this constructor like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a utilizar una sola capa deinterna de dimension 100\n",
    "hidden_layers = (100,)\n",
    "\n",
    "#Red Neuronal\n",
    "q_network = QNetwork(\n",
    "    #Numero de estados\n",
    "    train_env.observation_spec(),\n",
    "    #Numero de acciones\n",
    "    train_env.action_spec(),\n",
    "    #Configuracion de las capas de la RN\n",
    "    fc_layer_params=hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DqnAgent\n",
    "\n",
    "We define only one hidden layer, with 100 neurons and pass on information about training environment to the QNetwork constructor. Now we can instantiate an object of DQNAgent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Spec (Trajectory):\n",
      "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))\n",
      "Agent Spec Trajectory.observation:\n",
      "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "Agent Spec Trajectory.action:\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64))\n",
      "Agent Spec Trajectory.reward:\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n"
     ]
    }
   ],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "agent = DqnAgent(\n",
    "    # Entornos de prueba y de validacion\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    #Red Neuronal\n",
    "    q_network = q_network,\n",
    "    #Optimizador y funcion de error\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "    td_errors_loss_fn = common.element_wise_squared_loss,\n",
    "    #Numero de pasos por simulacion\n",
    "    train_step_counter = counter)\n",
    "\n",
    "#Inicializa el agente\n",
    "agent.initialize()\n",
    "\n",
    "print('Agent Spec (Trajectory):')\n",
    "print(agent.collect_data_spec)\n",
    "\n",
    "print('Agent Spec Trajectory.observation:')\n",
    "print(agent.collect_data_spec.observation)\n",
    "\n",
    "print('Agent Spec Trajectory.action:')\n",
    "print(agent.collect_data_spec.action)\n",
    "\n",
    "print('Agent Spec Trajectory.reward:')\n",
    "print(agent.collect_data_spec.reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulaciones\n",
    "\n",
    "The method bellow is used for calculations of how much reword has agent gained on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecuta un numero de simulaciones - episodes - y recupera el valor medio\n",
    "def get_average_return(environment, policy, episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    \n",
    "    #Para cada episodio\n",
    "    for _ in range(episodes):\n",
    "        #Reseteamos el entorno\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        #Empieza el episodio...\n",
    "        while not time_step.is_last():\n",
    "            #Ejecuta la accion determinada por la policy para el estado especificado\n",
    "            action_step = policy.action(time_step)\n",
    "            #Ejecuta la accion\n",
    "            time_step = environment.step(action_step.action)\n",
    "            #Actualiza la recompensa\n",
    "            episode_return += time_step.reward\n",
    "    \n",
    "        #Actualiza el total\n",
    "        total_return += episode_return\n",
    "    \n",
    "    #El retorno medio\n",
    "    avg_return = total_return / episodes\n",
    "    \n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "Ok, let’s build the last piece of the Deep Q-Learning ecosystem – Experience Replay. For this purpose, we implement the class with the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReply(object):\n",
    "    \n",
    "    def __init__(self, agent, enviroment):\n",
    "        #Lista de vectores. Se podran utilizar para el aprendizaje\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            #Estado\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=enviroment.batch_size,\n",
    "            #Tamaño de la lista\n",
    "            max_length=50000)\n",
    "        \n",
    "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                enviroment.action_spec())\n",
    "        \n",
    "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
    "        \n",
    "        self.dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, \n",
    "            sample_batch_size=BATCH_SIZE, \n",
    "            num_steps=2).prefetch(3)\n",
    "\n",
    "        self.iterator = iter(self.dataset)\n",
    "    \n",
    "    def _fill_buffer(self, enviroment, policy, steps):\n",
    "        for _ in range(steps):\n",
    "            self.timestamp_data(enviroment, policy)\n",
    "            \n",
    "    def timestamp_data(self, environment, policy):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        self._replay_buffer.add_batch(timestamp_trajectory)\n",
    "\n",
    "experience_replay = ExperienceReply(agent, train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(agent.collect_data_spec)\n",
    "print(train_env.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the constructor of this class, we initialize replay buffer, which is an object of the class __TFUniformReplayBuffer__.\n",
    "\n",
    "If your agent is not getting good results, you can play with batch size and length of the buffer.\n",
    "\n",
    "Also, we created and instance of RandomTFPolicy. This one is used to fill buffer with initial values, which is done by calling internal function _fill_buffer. This method in turn calls timestamp_data method for each state of the environment.\n",
    "\n",
    "Method timestamp_data then forms trajectory from the current state and the action defined by policy. This trajectory is stored in the the buffer.\n",
    "\n",
    "Final step of the constructor is to create an iterable tf.data.Dataset pipeline which feeds data to the agent.\n",
    "\n",
    "# Training and Evaluation\n",
    "\n",
    "Once we have all this prepared, implementing training process is straight forward:\n",
    "\n",
    "First, we initialize counter on the agent to 0 and get initial average return of reward. Then training process starts for defined number of iterations. During this process we first collect data from the environment and then use that data to train the agent’s both neural networks. We also periodically print out average reward return and loss on evaluation environment. Here is how that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 - Average Return = 23.0, Loss = 21.123376846313477.\n",
      "Iteration 2000 - Average Return = 31.600000381469727, Loss = 13.086341857910156.\n",
      "Iteration 3000 - Average Return = 32.20000076293945, Loss = 28.730281829833984.\n",
      "Iteration 4000 - Average Return = 126.5999984741211, Loss = 6.088086128234863.\n",
      "Iteration 5000 - Average Return = 161.0, Loss = 29.583511352539062.\n",
      "Iteration 6000 - Average Return = 188.3000030517578, Loss = 98.26377868652344.\n",
      "Iteration 7000 - Average Return = 200.0, Loss = 14.179779052734375.\n",
      "Iteration 8000 - Average Return = 199.89999389648438, Loss = 394.890380859375.\n",
      "Iteration 9000 - Average Return = 200.0, Loss = 198.0342254638672.\n",
      "Iteration 10000 - Average Return = 200.0, Loss = 263.45770263671875.\n",
      "Iteration 11000 - Average Return = 200.0, Loss = 656.7501831054688.\n",
      "Iteration 12000 - Average Return = 200.0, Loss = 70.80119323730469.\n",
      "Iteration 13000 - Average Return = 200.0, Loss = 55.57476806640625.\n",
      "Iteration 14000 - Average Return = 200.0, Loss = 62.50963592529297.\n",
      "Iteration 15000 - Average Return = 200.0, Loss = 112.1136474609375.\n",
      "Iteration 16000 - Average Return = 200.0, Loss = 1353.5341796875.\n",
      "Iteration 17000 - Average Return = 200.0, Loss = 94.29739379882812.\n",
      "Iteration 18000 - Average Return = 200.0, Loss = 123.21041870117188.\n",
      "Iteration 19000 - Average Return = 200.0, Loss = 73.7961654663086.\n",
      "Iteration 20000 - Average Return = 200.0, Loss = 134.7241668701172.\n"
     ]
    }
   ],
   "source": [
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = get_average_return(evaluation_env, agent.policy, EVAL_EPISODES)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(NUMBER_ITERATION):\n",
    "    \n",
    "    for _ in range(COLLECTION_STEPS):\n",
    "        experience_replay.timestamp_data(train_env, agent.collect_policy)\n",
    "\n",
    "    experience, info = next(experience_replay.iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    if agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
    "        avg_return = get_average_return(evaluation_env, agent.policy, EVAL_EPISODES)\n",
    "        print('Iteration {0} - Average Return = {1}, Loss = {2}.'.format(agent.train_step_counter.numpy(), avg_return, train_loss))\n",
    "        returns.append(avg_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
