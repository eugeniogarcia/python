{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsLTHlVdiZP3"
   },
   "outputs": [],
   "source": [
    "# Note: If you haven't installed tf-agents yet, run:\n",
    "try:\n",
    "  %%tensorflow_version 2.x\n",
    "except:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEgSa5qGdItD"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdvop99JlYSM"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5afbcec67b2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_agents'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31uij8nIo5bG"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqFn7q5bs3BF"
   },
   "source": [
    "In Reinforcement  Learning terminology, policies map an observation from the environment to an action or a distribution over actions. __In TF-Agents__, __observations__ from the environment are contained in a __named tuple `TimeStep('step_type', 'discount', 'reward', 'observation')`__, and __policies map timesteps to actions__ or distributions over actions. Most policies use  `timestep.observation`, some policies use `timestep.step_type` (e.g. to reset the state at the beginning of an episode in stateful policies), but `timestep.discount` and `timestep.reward` are usually ignored.\n",
    "\n",
    "Policies are related to other components in TF-Agents in the following way. Most policies have a neural network to compute actions and/or distributions over actions from TimeSteps. Agents can contain one or more policies for different purposes, e.g. a main policy that is being trained for deployment, and a noisy policy for data collection. Policies can be saved/restored, and can be used indepedently of the agent for data collection, evaluation etc.\n",
    "\n",
    "Some policies are easier to write in Tensorflow (e.g. those with a neural network), whereas others are easier to write in Python (e.g. following a script of actions). So in TF agents, we allow both Python and Tensorflow policies. Morever, policies written in TensorFlow might have to be used in a Python environment, or vice versa, e.g. a TensorFlow policy is used for training but later deployed in a production python environment. To make this easier, we provide wrappers for converting between python and TensorFlow policies.\n",
    "\n",
    "Another interesting class of policies are policy wrappers, which modify a given policy in a certain way, e.g. add a particular type of noise, make a greedy or epsilon-greedy version of a stochastic policy, randomly mix multiple policies etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En resumen, las policies toman como entrada un __time_Step__ y retornan un __policy_step__.\n",
    "\n",
    "Las policies las podemos clasificar en base a como estan implementadas, como python o tensorflow policies. En los imports podemos ver algunos ejemplos de policies de cada tipo:\n",
    "\n",
    "```py\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyXO5-Aalb-6"
   },
   "source": [
    "# Python Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOtUZ1hs02bu"
   },
   "source": [
    "The interface for Python policies is defined in `policies/py_policy.Base`. The main methods are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PqNEVls1uqc"
   },
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def __init__(self, time_step_spec, action_spec, policy_state_spec=()):\n",
    "    self._time_step_spec = time_step_spec\n",
    "    self._action_spec = action_spec\n",
    "    self._policy_state_spec = policy_state_spec\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def reset(self, policy_state=()):\n",
    "    # return initial_policy_state.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def action(self, time_step, policy_state=()):\n",
    "    # return a PolicyStep(action, state, info) named tuple.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def distribution(self, time_step, policy_state=()):\n",
    "    # Not implemented in python, only for TF policies.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def update(self, policy):\n",
    "    # update self to be similar to the input `policy`.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def copy(self):\n",
    "    # return a copy of self.\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def time_step_spec(self):\n",
    "    return self._time_step_spec\n",
    "\n",
    "  @property\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  @property\n",
    "  def policy_state_spec(self):\n",
    "    return self._policy_state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16kyDKk65bka"
   },
   "source": [
    "The most important method is `action(time_step)` which maps a `time_step` containing an observation from the environment to a PolicyStep named tuple containing the following attributes:\n",
    "\n",
    "*  `action`: The action to be applied to the environment.\n",
    "*  `state`: The state of the policy (e.g. RNN state) to be fed into the next call to action.\n",
    "*  `info`: Optional side information such as action log probabilities.\n",
    "\n",
    "The `time_step_spec` and `action_spec` are specifications for the input time step and the output action. Policies also have a `reset` function which is typically used for resetting the state in stateful policies. The `copy` function returns a copy of `self` and the `update(new_policy)` function updates `self` towards `new_policy`.\n",
    "\n",
    "Now, let us look at a couple of examples of python policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "La clase base que implementa una policiy es la que se describe en esta seccion. Sin embargo hay algunos metodos que solo tienen sentido, y que por lo tanto solo se implementan, dependiendo del tipo de policy.\n",
    "\n",
    "En python polities se implementara `action`, pero en tensorflow policies `distribution`. `update` y `copy` creo que solo tienen sentido en politicas tensorflow, porque despues de todo, solo se adiestran politicas tensorflow.\n",
    "\n",
    "El __policy_step__ tiene tres propiedades, pero la que realmente es util es la primera, __action__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCH1Hs_WlmDT"
   },
   "source": [
    "## Example 1: Random Python Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbnQ0BQ3_0N2"
   },
   "source": [
    "A simple example of a `PyPolicy` is the `RandomPyPolicy` which generates random actions for the discrete/continuous given action_spec. The input `time_step` is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QX8M4Nl-_0uu"
   },
   "outputs": [],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
    "    action_spec=action_spec)\n",
    "\n",
    "time_step = None\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)\n",
    "\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Usamos la policy __random_py_policy__. En esta policy realmente no tiene importancia el estado de partida, asi que pasamos None como time_step.\n",
    "\n",
    "Interesante ver tambien como al crear la policiy hay que especificar el tipo de la time_spec y de la action_spec (que se correspondera con la propiedad action de la policy_step). Para definir la spec en agents tenemos tambien varios helpers:\n",
    "\n",
    "```py\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "```\n",
    "\n",
    "En este caso decimos que la action sera un vector (2 filas y 1 columna) de enteros. Los valores maximos que pueden tomar los enteros son -10, 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8WrFOR1lz31"
   },
   "source": [
    "## Example 2: Scripted Python Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJ0Br1lGBnTT"
   },
   "source": [
    "A scripted policy plays back a script of actions represented as a list of `(num_repeats, action)` tuples. Every time the `action` function is called, it returns the next action from the list until the specified number of repeats is done, and then moves on to the next action in the list. The `reset` method can be called to start executing from the beginning of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mZ244m4BUYv"
   },
   "outputs": [],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeates` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=None, action_spec=action_spec, action_script=action_script)\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('Resetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Tenemos una action_spec como la del ejemplo anterior. Como en el caso anterior la policy no dependenra del estado de entrada. Retornara acciones de forma programada. La programacion se indica en un array:\n",
    "\n",
    "```py\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeates` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "```\n",
    "\n",
    "Estamos diciendo que la primera accion sera [5,2], seguida de [1,2], [1,2],[3,4]. De ahi volvera a comenzar la secuencia. __Notese__ que el primer digito indica el numero de repeticiones de la accion. Cuando especificamos `0` la accion se salta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Dz7HSTZl6aU"
   },
   "source": [
    "# TensorFlow Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwcoBXqKl8Yb"
   },
   "source": [
    "TensorFlow policies follow the same interface as Python policies. Let us look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x8pDWEFrQ5C"
   },
   "source": [
    "## Example 1: Random TF Policy\n",
    "\n",
    "A RandomTFPolicy can be used to generate random actions according to a given discrete/continuous `action_spec`. The input `time_step` is ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ3pe5G4rjrW"
   },
   "outputs": [],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "\n",
    "time_step = ts.restart(observation)\n",
    "\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOBoWETprWCB"
   },
   "source": [
    "## Example 2: Actor Policy\n",
    "\n",
    "An actor policy can be created using either a network that maps `time_steps` to actions or a network that maps `time_steps` to distributions over actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S94E5zQgge_"
   },
   "source": [
    "### Using an action network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2LM5STNgv1u"
   },
   "source": [
    "Let us define a network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2wFgzJFteQX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9b1302ae0a83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mActionNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     super(ActionNet, self).__init__(\n\u001b[0;32m      5\u001b[0m         \u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "class ActionNet(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "    super(ActionNet, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name='ActionNet')\n",
    "    self._output_tensor_spec = output_tensor_spec\n",
    "    self._layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "    ]\n",
    "\n",
    "  def call(self, observations, step_type, network_state):\n",
    "    del step_type\n",
    "\n",
    "    output = tf.cast(observations, dtype=tf.float32)\n",
    "    for layer in self.layers:\n",
    "      output = layer(output)\n",
    "    actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "    # Scale and shift actions to the correct range if necessary.\n",
    "    return actions, network_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Aqui simplemente hemos definido una NN. La NN toma una __observations__, __step_type__ y __network_state__ como entrada y retorna __actions__ y __network_state__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7fIn-ybVdC6"
   },
   "source": [
    "In TensorFlow most network layers are designed for batch operations, so we expect the input time_steps to be batched, and the output of the network will be batched as well. Also the network is responsible for producing actions in the correct range of the given action_spec. This is conventionally done using e.g. a tanh activation for the final layer to produce actions in [-1, 1] and then scaling and shifting this to the correct range as the input action_spec (e.g. see `tf_agents/agents/ddpg/networks.actor_network()`).\n",
    "\n",
    "Now, we an create an actor policy using the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UGmFTe7a5VQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e8bf7d887573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_tensor_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtime_step_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m action_spec = tensor_spec.BoundedTensorSpec((3,),\n\u001b[0;32m      4\u001b[0m                                             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                             \u001b[0mminimum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor_spec' is not defined"
     ]
    }
   ],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Se crea en primer lugar la NN. En este caso la entrada - aka, __observacion__ - es un vector de cuatro floats. La salida - aka accion -, es un vector de 3 floats comprendidos entre -1 y 1. Notese como ambos son un tensor:\n",
    "\n",
    "```py\n",
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "```\n",
    "\n",
    "Una vez la NN es instanciada:\n",
    "\n",
    "```py\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "```\n",
    "\n",
    "Se procede a crear la policy - `una ActorPolicy`:\n",
    "\n",
    "```py\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlmGPTAmfPK3"
   },
   "source": [
    "We can apply it to any batch of time_steps that follow time_step_spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvsIsR0VfOA4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_step_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-32f2922c2ae1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_step_spec' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "observations = tf.ones([2] + time_step_spec.observation.shape.as_list())\n",
    "\n",
    "time_step = ts.restart(observations, batch_size)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Notese como usamos el metodo `distribution`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lumtyhejZOXR"
   },
   "source": [
    "In the above example, we created the policy using an action network that produces an action tensor. In this case, `policy.distribution(time_step)` is a deterministic (delta) distribution around the output of `policy.action(time_step)`. One way to produce a stochastic policy is to wrap the actor policy in a policy wrapper that adds noise to the actions. Another way is to create the actor policy using an action distribution network instead of an action network as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eNrJ5gKgl3W"
   },
   "source": [
    "### Using an action distribution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSYzC9LobVsK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ActionNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-13396d268589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mActionDistributionNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActionNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     action_means, network_state = super(ActionDistributionNet, self).call(\n\u001b[0;32m      5\u001b[0m         observations, step_type, network_state)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ActionNet' is not defined"
     ]
    }
   ],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "\n",
    "  def call(self, observations, step_type, network_state):\n",
    "    action_means, network_state = super(ActionDistributionNet, self).call(\n",
    "        observations, step_type, network_state)\n",
    "\n",
    "    action_std = tf.ones_like(action_means)\n",
    "    return tfp.distributions.Normal(action_means, action_std), network_state\n",
    "\n",
    "\n",
    "action_distribution_net = ActionDistributionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_distribution_net)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzoNGJnlibtz"
   },
   "source": [
    "Note that in the above, actions are clipped to the range of the given action spec [-1, 1]. This is because a constructor argument of ActorPolicy clip=True by default. Setting this to false will return unclipped actions produced by the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLj6A-5domNG"
   },
   "source": [
    "Stochastic policies can be converted to deterministic policies using, for example, a GreedyPolicy wrapper which chooses `stochastic_policy.distribution().mode()` as its action, and a deterministic/delta distribution around this greedy action as its `distribution()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xxzo2a7rZ7v"
   },
   "source": [
    "## Example 3: Q Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79eGLqpOhQVp"
   },
   "source": [
    "A Q policy is used in agents like DQN and is based on a Q network that predicts a Q value for each discrete action. For a given time step, the action distribution in the Q Policy is a categorical distribution created using the q values as logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Haakr2VvjqKC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9911ba3a4878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_tensor_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtime_step_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m action_spec = tensor_spec.BoundedTensorSpec((1,),\n\u001b[0;32m      4\u001b[0m                                             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                             \u001b[0mminimum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor_spec' is not defined"
     ]
    }
   ],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((1,),\n",
    "                                            tf.int32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "\n",
    "class QNetwork(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, action_spec, num_actions=2, name=None):\n",
    "    super(QNetwork, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name=name)\n",
    "    self._layers.append(tf.keras.layers.Dense(num_actions))\n",
    "\n",
    "  def call(self, inputs, step_type=None, network_state=()):\n",
    "    del step_type\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    for layer in self.layers:\n",
    "      inputs = layer(inputs)\n",
    "    return inputs, network_state\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "observation = tf.ones([batch_size] + time_step_spec.observation.shape.as_list())\n",
    "time_steps = ts.restart(observation, batch_size=batch_size)\n",
    "\n",
    "my_q_network = QNetwork(\n",
    "    input_tensor_spec=input_tensor_spec,\n",
    "    action_spec=action_spec)\n",
    "my_q_policy = q_policy.QPolicy(\n",
    "    time_step_spec, action_spec, q_network=my_q_network)\n",
    "action_step = my_q_policy.action(time_steps)\n",
    "distribution_step = my_q_policy.distribution(time_steps)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Conceptualmente no veo diferencias en como hemos procedido con el Actor Agent y el Q Agent.\n",
    "\n",
    "En ambos casos se crea el agente pasando las specs de la observacion y de la accion, e indicando una red neuronal. \n",
    "\n",
    "En el caso del Q Agent la NN generara una distribucion de probabilidad - logits -. En esencia estamos tomando la RNN que determina Q, es decir, la valor esperado en cada estado - entrada - para cada una de las acciones -salida. La policy tomara la accion que maximice el valor - argmax.\n",
    "\n",
    "En el caso del Actor Agent la NN directamente nos determina - deterministico - la accion que hay que tomar - tiene como activacion una tahn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xpu9m6mvqJY-"
   },
   "source": [
    "# Policy Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfaUrqRAoigk"
   },
   "source": [
    "A policy wrapper can be used to wrap and modify a given policy, e.g. add noise. Policy wrappers are a subclass of Policy (Python/TensorFlow) and can therefore be used just like any other policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JJVVAALqVNQ"
   },
   "source": [
    "## Example: Greedy Policy\n",
    "\n",
    "\n",
    "A greedy wrapper can be used to wrap any TensorFlow policy that implements `distribution()`. `GreedyPolicy.action()` will return `wrapped_policy.distribution().mode()` and `GreedyPolicy.distribution()` is a deterministic/delta distribution around `GreedyPolicy.action()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsRPBeLZtXvu"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'greedy_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-11d9647d6b62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_greedy_policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgreedy_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGreedyPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_q_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_greedy_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Action:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'greedy_policy' is not defined"
     ]
    }
   ],
   "source": [
    "my_greedy_policy = greedy_policy.GreedyPolicy(my_q_policy)\n",
    "\n",
    "action_step = my_greedy_policy.action(time_steps)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_greedy_policy.distribution(time_steps)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "En esencia una policy se aplica sobre otra. En este caso usamos una Greedy policy - que es determinista - para obtener de forma determinista la accion que ha producido una policy stocastica."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "TF-Agents Policies Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
